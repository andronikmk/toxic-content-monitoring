<p align="center">
    <br>
    <img src="https://raw.githubusercontent.com/andronikmk/toxic-content-monitoring/master/docs/_static/Big-Armor-Logo.png" width="142"/>
    <br>
<p>

<p align="center">
    <a href="https://github.com/andronikmk/toxic-content-monitoring/blob/master/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/badge/License-MIT-yellow.svg">
    </a>
</p>
While there exist systems for detecting toxicity, suicidality, and other concerning behaviors, 
they are all either whole-system programs or are limited to only a small number of topics. 
This project creates an API that can be integrated into an applications used by anyone, such as 
a social worker, to detect concerning behaviors. It gives the end-user the ability to send 
information to the API, and get meaningful results.
